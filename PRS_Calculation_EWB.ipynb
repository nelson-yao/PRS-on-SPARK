{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prerpcoessing\n",
    "\n",
    "MAVAN genotypes are previously pruned using plink 1.90. With windows size set at 50bp, sliding step set at 5bp, and VIF threshold set at 2.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0 (default, Nov 12 2015, 10:55:08) \n",
      "[GCC 4.8.3]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x2b1a842b84e0>\n",
      "<pyspark.sql.context.SQLContext object at 0x2b1a65080f28>\n"
     ]
    }
   ],
   "source": [
    "# start pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"App\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext=SQLContext(sc)\n",
    "print(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import add \n",
    "from math import log\n",
    "# PRS calculation on pruned NFP data, 50, 5, VIF=2\n",
    "\n",
    "#**ATTN: python index starts at 0, so if you want to specify the second column, use 1\n",
    "#**ATTN: please remove the header of the GWAS file if there is any\n",
    "\n",
    "# define column number for contents in GWAS\n",
    "\n",
    "gwas_id=0    # column of SNP ID\n",
    "gwas_p=7     # column of P value\n",
    "gwas_or=5    # column of odds ratio\n",
    "gwas_a1=3    # column of a1 in the GWAS\n",
    "gwas_maf=4\n",
    "# defin column number for contents in genfile\n",
    "geno_id=1  # column number with rsID\n",
    "geno_start=5 # column number of the 1st genotype\n",
    "geno_a1 = 3  # column number that contains the reference allele\n",
    "\n",
    "# List of thresholds:\n",
    "thresholds=[0.5, 0.3, 0.2, 0.1, 0.05, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# file delimiters:\n",
    "GWAS_delim=\"\\t\"\n",
    "GENO_delim=\" \"\n",
    "log_or=False\n",
    "# file names:\n",
    "home=\"/home/nyao111/\"  #define homefolder path\n",
    "gwasFiles=home+\"PRS_imputed/pgc.cross.noheader.txt\"       # Name of GWAS file \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from PRSfunctions import filterGWASByP, makeGenotypeCheckRef, multiplyOdds, calcPRSFromGeno, getSampleNames, getMaf, labelPRS, writePRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculat PRS for Emotional Well-being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/vvp-220-aa/\r\n"
     ]
    }
   ],
   "source": [
    "!echo $SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home=\"/scratch/vvp-220-aa/\"\n",
    "EWBgwas=sc.textFile(home+\"GWAS-EWB-noheader-corrected-2.txt\")\n",
    "EWBgwastable=EWBgwas.map(lambda line: line.split(\"\\t\"))     \n",
    "EWBgwastable.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#== calculate genotype amd fiilter based on EWB GWAS\n",
    "EWBoddspair=EWBgwastable.map(lambda line: (line[0], float(line[5]))).collectAsMap()\n",
    "EWBGWAStop=EWBgwastable.map(lambda line: (line[0], line[3])).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EWBgwasmap=EWBgwastable.map(lambda line: (line[gwas_id], line)).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import re\n",
    "\n",
    "fileList={}\n",
    "for filename in os.listdir(home+\"MAVAN_imputed\"):\n",
    "    if filename.endswith(\".impute2\"):\n",
    "        chrom=int(re.search(\"(?<=chr).*(?=.pos)\", filename).group(0))\n",
    "        start=int(re.search(\"(?<=pos).*(?=-)\", filename).group(0))\n",
    "        #end=int(re.search(\"(?<=-).*(?=.impu)\", filename).group(0))\n",
    "        fileList[(chrom, start)]=filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rs3756290', (5, 130000001)), ('rs2075677', (20, 45000001)), ('rs4958581', (5, 150000001)), ('rs2572431', (8, 10000001)), ('rs193236081', (17, 40000001)), ('rs10960103', (9, 10000001)), ('rs4938021', (11, 110000001)), ('rs139237746', (11, 10000001)), ('rs1557341', (18, 35000001)), ('rs12938775', (17, 1)), ('rs12961969', (18, 35000001)), ('rs35688236', (3, 30000001)), ('rs2150462', (9, 20000001)), ('rs12903563', (15, 75000001)), ('rs7973260', (12, 115000001)), ('rs62100776', (18, 50000001)), ('rs4346787', (6, 25000001)), ('rs4481363', (5, 160000001)), ('rs10838738', (11, 45000001)), ('rs10774909', (12, 115000001)), ('rs6904596', (6, 25000001)), ('rs4481363', (5, 160000001))]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "posPair=EWBgwastable.map(lambda line: (line[0],(int(line[1]), int(line[2].strip('\"').replace(',', ''))//5000000*5000000+1))).collect()\n",
    "print (posPair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seekFile(positionList, fileMap):\n",
    "    result=[]\n",
    "    try:\n",
    "        for position in positionList:\n",
    "            result.append(fileMap[position[1]])\n",
    "    except:\n",
    "        print (position[0])\n",
    "        pass\n",
    "\n",
    "    return result\n",
    "\n",
    "inputList=seekFile(posPair, fileList)\n",
    "inputPaths=set([home+\"MAVAN_imputed/\"+filename for filename in inputList])\n",
    "len(inputPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EWBgeno=sc.textFile(\",\".join(inputPaths)).map(lambda line: line.split(' '))\n",
    "EWBgenoCA=EWBgeno.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EWBgenoFilterByGWASCA=EWBgenoCA.filter(lambda line: line[1] in EWBgwasmap).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EWBgenoFilterByGWASList=EWBgenoFilterByGWASCA.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EWBgenoAmbi=EWBgenoFilterByGWASCA.filter(lambda line: line[3]==bpMap[line[4]]).map(lambda line: (line[1], line[3], line[4])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rs10774909', 'C', 'G'),\n",
       " ('rs10960103', 'C', 'G'),\n",
       " ('rs62100776', 'A', 'T'),\n",
       " ('rs2150462', 'G', 'C')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EWBgenoAmbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EWBsnpAmbiSet=set(EWBgenoFilterByGWASCA.filter(lambda line: line[3]==bpMap[line[4]]).map(lambda line: line[1]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rs10960103', '9', '\"11,699,270\"', 'C', '0.77', '0.0264', '0.0042', '0.024%', '2.14E-10', '\"165,380\"', 'DS*']\n",
      "['rs2150462', '9', '\"23,316,330\"', 'C', '0.74', '-0.0217', '0.0039', '0.018%', '2.66E-08', '\"170,907\"', '']\n",
      "['rs62100776', '18', '\"50,754,633\"', 'A', '0.56', '-0.0252', '0.0044', '0.031%', '8.45E-09', '\"105,739\"', '\"DS**, SWB*\"']\n",
      "['rs10774909', '12', '\"117,674,129\"', 'C', '0.52', '0.0150', '0.0039', '0.011%', '1.20E-04', '0.0203', '\"131,235\"']\n"
     ]
    }
   ],
   "source": [
    "EWBgwasAmbi=EWBgwastable.filter(lambda line: line[0] in EWBsnpAmbiSet).collect()\n",
    "for line in EWBgwasAmbi:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## calculate allele frequencies for all snps\n",
    "alleleFreq=EWBgenoFilterByGWASCA.map(lambda line: getMaf(line, rsidCol=geno_id, start=geno_start, a1=geno_a1)).collectAsMap()\n",
    "#for line in alleleFreq.collect():\n",
    "   # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 16.0 failed 1 times, most recent failure: Lost task 7.0 in stage 16.0 (TID 599, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-38-17891718d6f0>\", line 2, in <lambda>\n  File \"/home/nyao111/PRSfunctions.py\", line 18, in makeGenotypeCheckRef\n    if rsid in strandMap:\nNameError: name 'strandMap' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-38-17891718d6f0>\", line 2, in <lambda>\n  File \"/home/nyao111/PRSfunctions.py\", line 18, in makeGenotypeCheckRef\n    if rsid in strandMap:\nNameError: name 'strandMap' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-17891718d6f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgeno_a1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEWBgenotype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEWBgenoFilterByGWASCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msnp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmakeGenotypeCheckRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqMap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malleleFreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgwasMap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEWBgwasmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeno_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1Col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeno_a1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeno_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgwasA1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgwas_a1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgwasMAF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgwas_maf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEWBgenotype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#EWBscore=calPRSFromGeno(EWBgenotype, EWBoddspair, totalcount=count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software6/apps/spark/2.0.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software6/apps/spark/2.0.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         \"\"\"\n\u001b[0;32m--> 999\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software6/apps/spark/2.0.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software6/apps/spark/2.0.0/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software6/apps/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software6/apps/spark/2.0.0/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/software6/apps/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 16.0 failed 1 times, most recent failure: Lost task 7.0 in stage 16.0 (TID 599, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-38-17891718d6f0>\", line 2, in <lambda>\n  File \"/home/nyao111/PRSfunctions.py\", line 18, in makeGenotypeCheckRef\n    if rsid in strandMap:\nNameError: name 'strandMap' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor52.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/software6/apps/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/software6/apps/spark/2.0.0/python/pyspark/rdd.py\", line 1008, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-38-17891718d6f0>\", line 2, in <lambda>\n  File \"/home/nyao111/PRSfunctions.py\", line 18, in makeGenotypeCheckRef\n    if rsid in strandMap:\nNameError: name 'strandMap' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "geno_a1=3\n",
    "EWBgenotype=EWBgenoFilterByGWASCA.map(lambda snp: makeGenotypeCheckRef(snp, freqMap=alleleFreq, gwasMap=EWBgwasmap, idCol=geno_id, a1Col=geno_a1, startCol=geno_start, gwasA1=gwas_a1, gwasMAF=gwas_maf))\n",
    "count=EWBgenotype.count()\n",
    "#EWBscore=calPRSFromGeno(EWBgenotype, EWBoddspair, totalcount=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv.register_dialect(\n",
    "    'outputdialect',\n",
    "    delimiter = ',',\n",
    "    lineterminator = '\\r\\n',\n",
    "    quoting = csv.QUOTE_NONE)\n",
    "import csv\n",
    "def parsePRS(scorelist, sampleSheetPath,idCol=4, dialect=None, skipline=0):\n",
    "    subjlist=[]\n",
    "    with open(sampleSheetPath, \"r\") as csvfile:\n",
    "        subjs=csv.reader(csvfile, dialect=dialect) \n",
    "        for idx, subject in enumerate(subjs):\n",
    "            if idx>skipline:\n",
    "                subjlist.append(subject[idCol])\n",
    "        return zip(subjlist, scorelist)\n",
    "\n",
    "\n",
    "def writePRS(scores, filePath, dialect=None):\n",
    "    with open(filePath, \"w\") as output:\n",
    "        prsWriter=csv.writer(output, dialect=dialect)\n",
    "        for score in scores:\n",
    "            prsWriter.writerow(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fullPRS=parsePRS(EWBscore, \"SubjectName_imputed_sorted.csv\")        \n",
    "writePRS(fullPRS, \"PRS_EWB_corrected_withproxySNPs.csv\",\"outputdialect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sampleSheet=pd.read_csv(\"SubjectName_imputed_sorted.csv\", header=0, sep=\",\")\n",
    "MAVANPRS_EWB=pd.DataFrame(EWBscore)\n",
    "MAVANPRS_EWB.columns=[\"PRS\"]\n",
    "MAVANPRS_EWB.index=sampleSheet.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAVANPRS_EWB.to_csv(\"MAVANPRS_Emotional_well-being.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate PRS for T2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gwasfile=sc.textFile(home+\"GWAS/DIAGRAMv3.2012DEC17.txt\")\n",
    "prunedsnp=sc.textFile(home+\"/MAVAN_prunedSNP.txt\")\n",
    "mavangeno=sc.textFile(home+\"MAVAN_imputed/*.impute2\")\n",
    "\n",
    "gwastable=gwasfile.map(lambda line: line.split(\"\\t\"))     \n",
    "gwaspairs=gwasfile.map(lambda line: (line.split(\"\\t\")[0], line))\n",
    "\n",
    "pruneList=prunedsnp.mapPartitions(buildlist).collect()\n",
    "pruneSet=set(pruneList)\n",
    "pruneLookup=sc.broadcast(pruneSet)\n",
    "\n",
    "GWASpruned=gwastable.filter(lambda line: line[0] in pruneLookup.value)  #292232 lines, distinct\n",
    "GWASprunedsnpID=GWASpruned.map(lambda snp: snp[0]).collect()\n",
    "GWASprunedsnpset=set(GWASprunedsnpID)\n",
    "\n",
    "GWASprunedsnpTop=GWASpruned.map(lambda snp: (snp[0], snp[3])).collectAsMap()\n",
    "\n",
    "genoPruned=mavangeno.filter(lambda line : line.split(' ')[1] in pruneLookup.value).map(lambda x: x.split(\" \"))\n",
    "GWASTopStrandBC=sc.broadcast(GWASprunedsnpTop)\n",
    "bpMap=sc.broadcast({\"C\":\"G\", \"G\":\"C\", \"A\":\"T\", \"T\":\"A\"})\n",
    "MAVANgenotype=genoPruned.map(lambda line:  makeGenotype(line, GWASTopStrandBC.value, bpMap.value))\n",
    "try:\n",
    "    test=MAVANgenotype.first()\n",
    "\n",
    "except:\n",
    "    print(\"Calculation Unccessful\")                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'rs2980319', u'T'), (u'rs3934834', u'C'), (u'rs3766191', u'C'), (u'rs3737728', u'G'), (u'rs9651273', u'G'), (u'rs6671356', u'C'), (u'rs6666280', u'T'), (u'rs4970405', u'G'), (u'rs9659914', u'C'), (u'rs9442373', u'C'), (u'rs2298217', u'T'), (u'rs4970357', u'C'), (u'rs9442380', u'C'), (u'rs4970358', u'A'), (u'rs9442385', u'T'), (u'rs3813200', u'C'), (u'rs2887286', u'T'), (u'rs6603781', u'A'), (u'rs715643', u'C'), (u'rs3818646', u'T'), (u'rs12563338', u'T'), (u'rs4018608', u'C'), (u'rs6668223', u'A'), (u'rs13306651', u'G'), (u'rs2296474', u'A'), (u'rs12403864', u'G'), (u'rs1571150', u'A'), (u'rs3766177', u'C'), (u'rs7531530', u'T'), (u'rs2296716', u'T'), (u'rs9439468', u'G'), (u'rs7540231', u'A'), (u'rs1107910', u'T'), (u'rs6603811', u'C'), (u'rs7545812', u'G'), (u'rs11585349', u'T'), (u'rs12408690', u'G'), (u'rs12141314', u'G'), (u'rs11260616', u'A'), (u'rs6663586', u'A'), (u'rs4648727', u'C'), (u'rs6692959', u'C'), (u'rs6681938', u'C'), (u'rs6664578', u'A'), (u'rs7511905', u'C'), (u'rs4648592', u'G'), (u'rs3855951', u'T'), (u'rs10907193', u'G'), (u'rs16824526', u'C'), (u'rs6688000', u'G')]\n"
     ]
    }
   ],
   "source": [
    "print(GWASpruned.map(lambda snp: (snp[0], snp[3])).take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalmissing=genoPruned.filter(lambda line: line[1] not in GWASprunedsnpTop).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalSNP=genoPruned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7202596"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalmissing ## 7202596 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7757777"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalSNP  #7757777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "genoPruned100=genoPruned.take(500)\n",
    "for geno in genoPruned100:\n",
    "    if geno[1] in GWASprunedsnpTop:\n",
    "        indicate=\"Exists\"\n",
    "    else:\n",
    "        indicate=\"Missing\"\n",
    "    print(geno[1]+\" \"+indicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'rs2980319', u'1', u'766985', u'T', u'A', u'3.9E-01', u'1.03', u'0.97', u'1.09', u'7575', u'27012'], [u'rs3934834', u'1', u'995669', u'C', u'T', u'4.3E-01', u'1.03', u'0.96', u'1.11', u'4752', u'33536'], [u'rs3766191', u'1', u'1007450', u'C', u'T', u'4.1E-01', u'1.04', u'0.95', u'1.13', u'4752', u'33536'], [u'rs3737728', u'1', u'1011278', u'G', u'A', u'7.3E-01', u'1.01', u'0.96', u'1.06', u'6634', u'49797'], [u'rs9651273', u'1', u'1021403', u'G', u'A', u'6.8E-01', u'1.01', u'0.96', u'1.07', u'4752', u'33536'], [u'rs6671356', u'1', u'1029889', u'C', u'T', u'3.6E-01', u'1.04', u'0.96', u'1.12', u'5426', u'41200'], [u'rs6666280', u'1', u'1036027', u'T', u'C', u'2.8E-01', u'1.04', u'0.97', u'1.12', u'6201', u'48359'], [u'rs4970405', u'1', u'1038818', u'G', u'A', u'3.4E-02', u'1.07', u'1.01', u'1.15', u'8792', u'51411'], [u'rs9659914', u'1', u'1051878', u'C', u'T', u'6.3E-01', u'1.04', u'0.89', u'1.22', u'4752', u'33536'], [u'rs9442373', u'1', u'1052501', u'C', u'A', u'4.9E-01', u'1.02', u'0.96', u'1.08', u'4752', u'33536']]\n"
     ]
    }
   ],
   "source": [
    "home+\"GWAS/DIAGRAMv3.2012DEC17.txt\"\n",
    "home+\"MAVAN_imputed/*.impute2\"\n",
    "home+\"/MAVAN_prunedSNP.txt\"\n",
    "\n",
    "MAVANgenoT2D, T2DGWAS=prepare(home+\"GWAS/DIAGRAMv3.2012DEC17-noheader.txt\",home+\"MAVAN_imputed/*.impute2\", home+\"/MAVAN_prunedSNP.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'rs58108140', None),\n",
       " (u'rs180734498', None),\n",
       " (u'chr1:28590', None),\n",
       " (u'rs140337953', None),\n",
       " (u'rs116400033', None),\n",
       " (u'rs141149254', None),\n",
       " (u'rs2462492', None),\n",
       " (u'rs143174675', None),\n",
       " (u'rs3091274', None),\n",
       " (u'rs200769871', None)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAVANgenoT2D.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''pruneCollect=pruneList.collect()\n",
    "pruneSet=set(pruneCollect)\n",
    "import pandas as pd\n",
    "import csv\n",
    "snpdata=pd.DataFrame(pruneCollect)\n",
    "snpdata.to_csv(home+\"PRS/prunedSNP.txt\", \\\n",
    "               header=None, index=None, sep=' ', quoting=csv.QUOTE_NONE, escapechar=' ')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pruneLookup=sc.broadcast(pruneSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'rs2980319',\n",
       " u'1',\n",
       " u'766985',\n",
       " u'T',\n",
       " u'A',\n",
       " u'3.9E-01',\n",
       " u'1.03',\n",
       " u'0.97',\n",
       " u'1.09',\n",
       " u'7575',\n",
       " u'27012']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GWASpruned=gwastable.filter(lambda line: line[0] in pruneLookup.value)  #292232 lines, distinct\n",
    "T2DGWASpruned=T2Dgwastable.filter(lambda line: line[0] in pruneLookup.value) \n",
    "T2DGWASpruned.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GWASprunedsnpID=GWASpruned.map(lambda snp: snp[0]).collect()\n",
    "GWASprunedsnpset=set(GWASprunedsnpID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T2DGWASprunedsnpID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "GWASprunedsnpZpair=GWASpruned.map(lambda snp: (snp[0], float(snp[5]))).collectAsMap()\n",
    "GWASprunedsnpTopStrand=GWASpruned.map(lambda snp: (snp[0], snp[3])).collectAsMap()\n",
    "\n",
    "#T2D GWAS ODDspairs and TopStrands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prunedsnpBC=sc.broadcast(GWASprunedsnpset)\n",
    "mavangeno=sc.textFile(home+\"MAVAN_imputed/*.impute2\")\n",
    "genoFilter=mavangeno.filter(lambda line : line.split(' ')[1] in prunedsnpBC.value).map(lambda x: x.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'---',\n",
       " u'rs12562034',\n",
       " u'768448',\n",
       " u'G',\n",
       " u'A',\n",
       " u'0.994',\n",
       " u'0.006',\n",
       " u'0',\n",
       " u'0.890',\n",
       " u'0.110']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genofirst=genoFilter.first()\n",
    "genofirst[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Genotype and PRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GWASprunedsnpZpairBC=sc.broadcast(GWASprunedsnpZpair)\n",
    "#  Make genotype in 0,1,2,format\n",
    "MAVANgenotype=genoFilter.map(lambda line: (line[1], makeGenotype(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read genotype from \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate scores first, then filter\n",
    "MAVANscores=MAVANgenotype.map(lambda snp : (snp[0], snp[1] * GWASprunedsnpZpairBC.value[snp[0]])).cache()\n",
    "GWASpvaluePair=GWASpruned.map(lambda line: (line[0], float(line[6])))\n",
    "GWASpvaluePairCache=GWASpvaluePair.cache()\n",
    "ADDSNP0_0001=set(GWASpvaluePairCache.filter(lambda line: line[1]<=0.0001).keys().collect())\n",
    "ADDSNP0_001=set(GWASpvaluePairCache.filter(lambda line: line[1]<=0.001).keys().collect())\n",
    "ADDSNP0_01=set(GWASpvaluePairCache.filter(lambda line: line[1]<=0.01).keys().collect())\n",
    "ADDSNP0_05=set(GWASpvaluePairCache.filter(lambda line: line[1]<=0.05).keys().collect())\n",
    "ADDSNP0_1=set(GWASpvaluePairCache.filter(lambda line: line[1]<=0.1).keys().collect())\n",
    "ADDSNP0_2=set(GWASpvaluePairCache.filter(lambda line: line[1]<=0.2).keys().collect())\n",
    "ADDSNP0_5=set(GWASpvaluePairCache.filter(lambda line: line[1]<=0.5).keys().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter the odds ratio first, apply each to genotype\n",
    "GWASprunedCA=GWASpruned.cache()\n",
    "ADDOdds0_0001=filterGWASByP(GWASprunedCA, 0.0001).collectAsMap()\n",
    "ADDOdds0_001=filterGWASByP(GWASprunedCA, 0.001).collectAsMap()\n",
    "ADDOdds0_01=filterGWASByP(GWASprunedCA, 0.01).collectAsMap()\n",
    "ADDOdds0_05=filterGWASByP(GWASprunedCA, 0.05).collectAsMap()\n",
    "ADDOdds0_1=filterGWASByP(GWASprunedCA, 0.1).collectAsMap()\n",
    "ADDOdds0_2=filterGWASByP(GWASprunedCA, 0.2).collectAsMap()\n",
    "ADDOdds0_5=filterGWASByP(GWASprunedCA, 0.5).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAVANgenotypeCache=MAVANgenotype.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PRS_ADD0_0001=calPRSFromGeno(MAVANgenotypeCache, ADDOdds0_0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PRS_ADD0_001=calPRSFromGeno(MAVANgenotypeCache, ADDOdds0_001)\n",
    "PRS_ADD0_01=calPRSFromGeno(MAVANgenotypeCache, ADDOdds0_01)\n",
    "PRS_ADD0_05=calPRSFromGeno(MAVANgenotypeCache, ADDOdds0_05)\n",
    "PRS_ADD0_1=calPRSFromGeno(MAVANgenotypeCache, ADDOdds0_1)\n",
    "PRS_ADD0_2=calPRSFromGeno(MAVANgenotypeCache, ADDOdds0_2)\n",
    "PRS_ADD0_5=calPRSFromGeno(MAVANgenotypeCache, ADDOdds0_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAVANgenotype.saveAsPickleFile(\"MavanGenotype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleSheet=pd.read_csv(\"SubjectName_imputed.csv\", header=0, sep=\",\")\n",
    "\n",
    "scores=np.array([PRS_ADD0_0001,PRS_ADD0_001, PRS_ADD0_01, PRS_ADD0_05, PRS_ADD0_1, PRS_ADD0_2, PRS_ADD0_5])\n",
    "scores.shape\n",
    "MAVANPRS_add=pd.DataFrame(scores, index=[\"P0.0001\", \"P0.001\", \"P0.01\", \"P0.05\", \"P0.1\", \"P0.2\", \"P0.5\"])\n",
    "MAVANPRS_add.columns=sampleSheet.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAVANPRS_add.transpose().to_csv(\"MAVANPRS_add_imputed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
