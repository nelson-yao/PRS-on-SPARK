{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from operator import add\n",
    "from math import log\n",
    "import csv\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter\n",
    "import re\n",
    "import glob, os\n",
    "\n",
    "import ntpath\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "from time import time\n",
    "import argparse\n",
    "from PRS_run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# type of files, VCF or GEN\n",
    "filetype=\"VCF\"\n",
    "\n",
    "# write snp log file or not \n",
    "snp_log= True\n",
    "## Setting parameters\n",
    "gwas_id=0 # column of SNP ID\n",
    "gwas_p=1     # column of P value\n",
    "gwas_or=2    # column of odds ratio\n",
    "gwas_a1=3    # column of a1 in the GWAS\n",
    "gwas_a2=4\n",
    "gwas_a1f=5  # column index of maf in the GWAS\n",
    "\n",
    "# defin column number for contents in genfile\n",
    "if filetype.lower()==\"vcf\":\n",
    "    chrom_id=0\n",
    "    bp_id=1\n",
    "    geno_id= 2 # column number with rsID\n",
    "    geno_start=9 # column number of the 1st genotype, in the raw vcf files, after separated by the delimiter of choice\n",
    "    geno_a1 = 3 # column number that contains the reference allele\n",
    "    GENO_delim= \"\\t\"\n",
    "    \n",
    "elif filetype.lower()==\"gen\":\n",
    "    chrom_id=0\n",
    "    geno_id = 1\n",
    "    bp_id=2\n",
    "    geno_start=5\n",
    "    geno_a1=3\n",
    "    GENO_delim= \" \"\n",
    "\n",
    "\n",
    "# List of thresholds:\n",
    "thresholds=[0.01, 0.05,0.1,0.2]\n",
    "threshold_seq=[0.001 ,0.01,0.002]\n",
    "            \n",
    "step=0.01  # default step size\n",
    "threshold_interval=[]\n",
    "if threshold_seq is not None:\n",
    "    if len(threshold_seq)==3:\n",
    "        lower=min(threshold_seq[0:2])\n",
    "        upper=max(threshold_seq[0:2])\n",
    "        step=threshold_seq[2]\n",
    "        threshold_interval=np.arange(lower, upper+step, step).tolist()\n",
    "    else:\n",
    "        raise(\"Invalid input for threshold sequence parameters\")\n",
    "        logger.error(\"Invalid input for threshold sequence parameters\")\n",
    "        \n",
    "thresholds=thresholds+threshold_interval\n",
    "# file delimiters:\n",
    "GWAS_delim=\"\\t\"\n",
    "\n",
    "# file names:\n",
    "#home=\"/Volumes/mavan/Genotyping_161114/MAVAN_imputed_161121/KIDS_info03/\"  #define homefolder path\n",
    "\n",
    "# Name of GWAS file\n",
    "gwasFiles=\"file:///home/meaney.lab/nyao/PRS/PRS/FirstTest/TestGWAS.txt\"\n",
    "GWAS_has_header=True\n",
    "\n",
    "# programme parameter\n",
    "log_or=False  # sepcify whether you want to log your odds ratios\n",
    "check_ref=True # if you know that there are mismatch between the top strand in the genotypes and that of the GWAS, set True. Not checking the reference allele will improve the speed\n",
    "use_maf=True   # whether to use MAF to check reference allele\n",
    "\n",
    "# sample file path and name\n",
    "sampleFilePath=\"../KIDS.sample\" # include the full/relative path and name of the sample file\n",
    "sampleFileDelim=\" \"  # sample File Delimiter\n",
    "sampleFileID=[0]   # which column in the sample file has the ID\n",
    "sample_skip=2  # how many lines to skip so that the sample names can be matched to the genotypes 1-to-1, taking into account the header of the sample file\n",
    "##output file information\n",
    "\n",
    "outputPath=\"../FirstTest/TestResult\"\n",
    "\n",
    "# Sepcify whether to check for duplicate SNPs\n",
    "checkDup=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the name of the genotype files\n",
    "genoFileNamePattern=\"file:///home/meaney.lab/nyao/PRS/PRS/FirstTest/Test.vcf\"\n",
    "if \"file:/\" in genoFileNamePattern:\n",
    "    genoFilePaths=re.sub(\"file://\", \"\", genoFileNamePattern)\n",
    "\n",
    "\n",
    "# get the whole list of the file names\n",
    "genoFileNames=glob.glob(genoFilePaths)\n",
    "\n",
    "# parameter for phenotype regression\n",
    "pheno_file=None\n",
    "#pheno_columns=results.pheno_columns\n",
    "#pheno_delim=results.pheno_delim\n",
    "#pheno_no_header=results.pheno_no_header\n",
    "#covar_columns=results.covar_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" configure logging control \"\"\"\n",
    "\n",
    "logger = logging.getLogger(\"Test\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# create file handler which logs even debug messages\n",
    "fh = logging.FileHandler(outputPath+\".log\")\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# create console handler with a higher log level\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# create formatter and add it to the handlers\n",
    "formatter1 = logging.Formatter('%(asctime)s %(levelname)s : %(message)s')\n",
    "formatter2 = logging.Formatter('%(asctime)s %(levelname)s : %(message)s')\n",
    "\n",
    "ch.setFormatter(formatter1)\n",
    "fh.setFormatter(formatter2)\n",
    "# add the handlers to the logger\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Reading Files\n",
      "Using these genoytpe files: \n",
      "/home/meaney.lab/nyao/PRS/PRS/FirstTest/Test.vcf\n",
      "total of 1 files\n"
     ]
    }
   ],
   "source": [
    "##  start spark context\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row\n",
    "APP_NAME=\"PRS\"\n",
    "\n",
    "spark=SparkSession.builder.appName(APP_NAME).config(\"spark.submit.pyFiles\", \"file:///home/meaney.lab/nyao/PRS/PRS/PRS-on-SPARK/PRS_run.py\").getOrCreate()\n",
    "\n",
    "# if using spark < 2.0.0, use the pyspark module to make Spark context\n",
    "# conf = pyspark.SparkConf().setAppName(APP_NAME).set()#.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "#sc  = SparkContext(\"spark://172.100.100.101:7077\")\n",
    "sc=spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"WARN\")\n",
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(__name__)\n",
    "print(\"Start Reading Files\")\n",
    "print(\"Using these genoytpe files: \")\n",
    "\n",
    "for filename in genoFileNames[:min(24, len(genoFileNames))]:\n",
    "    print(filename)\n",
    "if len(genoFileNames)>23:\n",
    "    print(\"and more...\")\n",
    "\n",
    "print(\"total of {} files\".format(str(len(genoFileNames))))\n",
    "# 1. Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GWAS file: file:///home/meaney.lab/nyao/PRS/PRS/FirstTest/TestGWAS.txt\n",
      "Showing top 5 rows of GWAS file\n",
      "+----------+-------+-------------------+---+---+---------+\n",
      "|       SNP|      P|               BETA| A1| A2|    CEUAF|\n",
      "+----------+-------+-------------------+---+---+---------+\n",
      "| rs3131967|0.06683| 0.0322157032979816|  T|  C|        .|\n",
      "|rs12562034| 0.8489|-0.0030070181092943|  A|  G|0.0925926|\n",
      "|rs12124819|0.05396| 0.0342272607705507|  A|  G|        .|\n",
      "| rs4970383|0.05491| 0.0277572046905535|  A|  C| 0.201835|\n",
      "| rs1806509|0.00927|-0.0338113190438628|  A|  C| 0.600917|\n",
      "+----------+-------+-------------------+---+---+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "System recognizes the following information in the GWAS :\n",
      "SNP ID : Column 0\n",
      "P-values : Column 1\n",
      "Effect size : Column 2\n",
      "Allele A1 : Column 3\n",
      "Allele A2 : Column 4\n",
      "Allele Frequencies : Column 5\n"
     ]
    }
   ],
   "source": [
    "# read the raw data\n",
    "genodata=sc.textFile(genoFileNamePattern)\n",
    "#print(\"Using the GWAS file: {}\".format(ntpath.basename(gwasFiles)))\n",
    "print(\"Using the GWAS file: {}\".format(gwasFiles))\n",
    "gwastable=spark.read.option(\"header\",GWAS_has_header).option(\"delimiter\", \"\\t\").csv(gwasFiles).cache()\n",
    "print(\"Showing top 5 rows of GWAS file\")\n",
    "gwastable.show(5)\n",
    "\n",
    "print(\"System recognizes the following information in the GWAS :\")\n",
    "print(\"SNP ID : Column {}\".format(gwas_id))\n",
    "print(\"P-values : Column {}\".format(gwas_p))\n",
    "print(\"Effect size : Column {}\".format(gwas_or))\n",
    "print(\"Allele A1 : Column {}\".format(gwas_a1))\n",
    "print(\"Allele A2 : Column {}\".format(gwas_a1+1))\n",
    "if use_maf:\n",
    "    print(\"Allele Frequencies : Column {}\".format(gwas_a1f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 10:37:43,531 INFO : Genotype data format : .VCF \n"
     ]
    }
   ],
   "source": [
    "# filter the genotype to contain only the SNPs less than the maximum p value threshold in the GWAS\n",
    "# Add a function that only keep the SNPs that match in chromosome and BP in geno and gwas\n",
    "\n",
    "maxThreshold=max(thresholds)  # maximum p value\n",
    "gwasOddsMapMax=filterGWASByP_DF(GWASdf=gwastable, pcolumn=gwas_p, idcolumn=gwas_id, oddscolumn=gwas_or, pHigh=maxThreshold, logOdds=log_or)\n",
    "gwasOddsMapMaxCA=sc.broadcast(gwasOddsMapMax).value  # Broadcast the map\n",
    "\n",
    "# ### 2. Initial processing\n",
    "# at this step, the genotypes are already filtered to keep only the ones in 'gwasOddsMapMax'\n",
    "bpMap={\"A\":\"T\", \"T\":\"A\", \"C\":\"G\", \"G\":\"C\"}\n",
    "tic=time.time()\n",
    "\n",
    "\n",
    "if filetype.lower()==\"vcf\":\n",
    "    logger.info(\"Genotype data format : .VCF \")\n",
    "\n",
    "    # Change to the format [snpid, A1, A2, *genotypelist]\n",
    "    genointermediate=genodata.filter(lambda line: (\"#\" not in line)).map(lambda line: line.split(GENO_delim)).filter(lambda line: line[geno_id] in gwasOddsMapMaxCA).map(lambda line:([line[x] for x in [geno_id, geno_a1, geno_a1+1]],[chunk.strip('\"').split(\":\")[3] for chunk in line[geno_start::]]))\\\n",
    "    .mapValues(lambda line:[float(x) for x in \",\".join(line).split(\",\")])  \n",
    "    \n",
    "elif filetype.lower() == \"gen\":\n",
    "    logger.info(\"Genotype data format : .GEN\")\n",
    "    # Change to the format [snpid, A1, A2, *genotypelist]\n",
    "    genointermediate=genodata\\\n",
    "    .filter(lambda line: line.split(GENO_delim)[geno_id] in gwasOddsMapMaxCA)\\\n",
    "    .map(lambda line: ([line.split(GENO_delim)[x] for x in [geno_id,geno_a1, geno_a1+1]], [float(x) for x in line.split(GENO_delim)[geno_start::]]))\n",
    "    \n",
    "\n",
    "# Change to the format [snpid, *genotypelist]\n",
    "genotable=genointermediate.map(lambda line: (line[0][0], line[1]))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'##fileformat=VCFv4.2']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genodata.first().split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-21 10:37:53,528 INFO : Determining strand alignment, using MAF\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 9, 172.100.100.101): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 161, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 56, in read_command\n    command = serializer.loads(command.value)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 422, in loads\n    return pickle.loads(obj)\nImportError: No module named PRS_run\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 161, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 56, in read_command\n    command = serializer.loads(command.value)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 422, in loads\n    return pickle.loads(obj)\nImportError: No module named PRS_run\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ca929b721697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_maf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Determining strand alignment, using MAF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mgenoA1f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenointermediate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgetA1f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Snpid_geno\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GenoA1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GenoA2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GenoA1f\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mgwasA1f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgwastable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgwas_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgwas_a1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgwas_a2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgwas_a1f\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Snpid_gwas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GwasA1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GwasA2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GwasA1f\"\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mchecktable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenoA1f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgwasA1f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenoA1f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Snpid_geno\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mgwasA1f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Snpid_gwas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \"\"\"\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/apps/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 9, 172.100.100.101): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 161, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 56, in read_command\n    command = serializer.loads(command.value)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 422, in loads\n    return pickle.loads(obj)\nImportError: No module named PRS_run\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 161, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 56, in read_command\n    command = serializer.loads(command.value)\n  File \"/share/apps/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 422, in loads\n    return pickle.loads(obj)\nImportError: No module named PRS_run\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "if check_ref:\n",
    "    if use_maf:\n",
    "        logger.info(\"Determining strand alignment, using MAF\")\n",
    "        genoA1f=genointermediate.map(lambda line: (line[0]+[getA1f(line[1])])).toDF([\"Snpid_geno\", \"GenoA1\", \"GenoA2\", \"GenoA1f\"])\n",
    "        gwasA1f=gwastable.rdd.map(lambda line:(line[gwas_id], line[gwas_a1], line[gwas_a2], line[gwas_a1f])).toDF([\"Snpid_gwas\", \"GwasA1\", \"GwasA2\", \"GwasA1f\" ])\n",
    "        checktable=genoA1f.join(gwasA1f, genoA1f[\"Snpid_geno\"]==gwasA1f[\"Snpid_gwas\"], \"inner\").cache()\n",
    "        if checkDup:\n",
    "            logger.info(\"Searching and removing duplicated SNPs\")\n",
    "            flagList = checktable.rdd.map(lambda line: checkAlignmentDF(line, bpMap)).collect()\n",
    "            flagMap = rmDup(flagList)\n",
    "        else:\n",
    "            flagMap = checktable.rdd.map(lambda line: checkAlignmentDF(line, bpMap)).collectAsMap()\n",
    "    else:\n",
    "        logger.info(\"Determining strand alignment, without using MAF. SNPs with Alleles that are reverse compliments will be discarded\")\n",
    "        genoalleles=genotable.map(lambda line: (line[0])).toDF([\"Snpid_geno\", \"GenoA1\", \"GenoA2\"])\n",
    "        gwasalleles=gwastable.rdd.map(lambda line:(line[gwas_id], line[gwas_a1], line[gwas_a2])).toDF([\"Snpid_gwas\", \"GwasA1\", \"GwasA2\"])\n",
    "        checktable=genoalleles.join(gwasalleles, genoalleles[\"Snpid_geno\"]==gwasalleles[\"Snpid_gwas\"], \"inner\").cache()\n",
    "\n",
    "        if checkDup:\n",
    "            logger.info(\"Searching and removing duplicated SNPs\")\n",
    "            flagList = checktable.rdd.map(lambda line: checkAlignmentDFnoMAF(line, bpMap)).collect()\n",
    "            flagMap = rmDup(flagList)\n",
    "        else:\n",
    "            flagMap = checktable.rdd.map(lambda line: checkAlignmentDFnoMAF(line, bpMap)).collectAsMap()\n",
    "\n",
    "    logger.info(\"Generating genotype dosage while taking into account difference in strand alignment\")\n",
    "    flagMap=sc.broadcast(flagMap).value\n",
    "    genotypeMax=genotable.filter(lambda line: line[0] in flagMap and flagMap[line[0]]!=\"discard\" ).map(lambda line: makeGenotypeCheckRef(line, checkMap=flagMap)).cache()\n",
    "\n",
    "else:\n",
    "    logger.info(\"Generating genotype dosage without checking allele alignments\")\n",
    "    genotypeMax=genotable.mapValues(lambda line: makeGenotype(line)).cache()\n",
    "    flagMap=False\n",
    "    if checkDup:\n",
    "        logger.info(\"Searching and removing duplicated SNPs\")\n",
    "        genotypeCount=genotypeMax.map(lambda line: (line[0], 1)).reduceByKey(lambda a,b: a+b).filter(lambda line: line[1]==1).collectAsMap()\n",
    "        genotypeMax=genotypeMax.filter(lambda line: line[0] in genotypeCount)\n",
    "\n",
    "logger.info(\"Dosage generated in {:.1f} seconds\".format(time.time()-tic) )\n",
    "samplesize=int(len(genotypeMax.first()[1]))\n",
    "logger.info(\"Detected {} samples in genotype data\" .format(str(samplesize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genotypeMax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-bef9daa39ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msnpBinRDD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnpBin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mgenotypeMaxRanked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msnpBinRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenotypeMax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'genotypeMax' is not defined"
     ]
    }
   ],
   "source": [
    "## use the thresholds as bins, put each snp in the corresponding bins\n",
    "\n",
    "gwasP=gwastable.rdd.filter(lambda line: float(line[gwas_p])< maxThreshold).map(lambda line: (line[gwas_id], float(line[gwas_p]))).collect()\n",
    "\n",
    "def binTuple(snpwithP, thresholdList):\n",
    "  results=[]\n",
    "  snpwithPsorted=sorted(snpwithP,key=lambda x: x[1])\n",
    "  thresholdSorted=sorted(thresholdList)\n",
    "  thresholdidx=0\n",
    "  for snp, p in snpwithPsorted:\n",
    "\n",
    "    if p>thresholdSorted[thresholdidx]:\n",
    "      thresholdidx+=1    \n",
    "    results.append((snp,thresholdSorted[thresholdidx]))\n",
    "  return results\n",
    "  \n",
    "snpBin=binTuple(gwasP, thresholds)\n",
    "\n",
    "snpBinRDD=sc.parallelize(snpBin)\n",
    "\n",
    "genotypeMaxRanked=snpBinRDD.join(genotypeMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#genoa1f.map(lambda line:\"\\t\".join([line[0], \"\\t\".join(line[1]), str(line[2])])).saveAsTextFile(\"../MOMS_info03_maf\")\n",
    "\n",
    "# Calculate PRS at the sepcified thresholds\n",
    "if flagMap:\n",
    "  genocalltable=genotable.filter(lambda line: line[0] in flagMap and flagMap[line[0]]!=\"discard\" ).mapValues(lambda geno: getCall(geno)).cache()\n",
    "else:\n",
    "  genocalltable=genotable.mapValues(lambda geno: getCall(geno))\n",
    "\n",
    "assert len(genocalltable.first()[1])==samplesize, \"Bug found, size of genotype and call table differ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'rs6870608', (0.05, [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "genocalltableRanked=snpBinRDD.join(genocalltable)\n",
    "\n",
    "## multiply each call by the odds\n",
    "## sum up the score, and the calls, within each rank\n",
    "\n",
    "\n",
    "def calcIntervals(genotypeRDDRanked, gwasOddsMap, calltableRanked, logsnpON, logger=logger):\n",
    "  logger.info(\"Calculating scores in each bin\")\n",
    "  genotypeRDDMultipled=genotypeRDDRanked.map(lambda line: (line[1][0], [x*gwasOddsMap[line[0]] for x in line[1][1]]))\n",
    "  intervalScoreRDD=genotypeRDDMultipled.reduceByKey(lambda snp1, snp2: map(add, snp1, snp2))\n",
    "  intervalScores=intervalScoreRDD.collect()\n",
    "  \n",
    "  logger.info(\"Calculating calls in each bin\")\n",
    "  intervalCallsRDD=calltableRanked.map(lambda line:line[1]).reduceByKey(lambda snp1, snp2: map(add, snp1, snp2))\n",
    "  \n",
    "  intervalCalls=intervalCallsRDD.collect()\n",
    "\n",
    "  logger.info(\"Generating snp list in each bin\")\n",
    "  \n",
    "  snpLists=False\n",
    "  if logsnpON:\n",
    "    snpLists=calltableRanked.map(lambda line:(line[1][0], line[0])).groupByKey().map(lambda line: (line[0], list(line[1]))).collect()\n",
    "    \n",
    "  return intervalScores, intervalCalls, snpLists\n",
    "\n",
    "scoresBin, callsBin, snpBin=calcIntervals(genotypeMaxRanked, gwasOddsMapMaxCA, genocalltableRanked, snp_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Take the sum of scores, calls and snplist in each bin and gather them\n",
    "def gatherScores(binScores,binCalls,binSNPs, thresholdList, logger=logger):\n",
    "  prsResults={}\n",
    "  snpNames={}\n",
    "  binScoresSorted=sorted(binScores)\n",
    "  binCallsSorted=sorted(binCalls)\n",
    "  binSNPsSorted=sorted(binSNPs)\n",
    "  logger.info(\"Start gathering scores from each bin\")\n",
    "  binThresholds=[x[0] for x in binScoresSorted]\n",
    "  for x in binThresholds:\n",
    "    if x not in thresholdList:\n",
    "      logger.info(\"No SNPs exist at threshold {}\".format(x))\n",
    "  \n",
    "  assert binThresholds==[x[0] for x in binCallsSorted], \"Error, scores and calls have different bins\"\n",
    "  assert binThresholds==[x[0] for x in binSNPsSorted], \"Error, scores and SNP list have different bins\"\n",
    "  \n",
    "  binScoresSortedvalues=[x[1] for x in binScoresSorted]\n",
    "  binCallsSortedvalues=[x[1] for x in binCallsSorted]\n",
    "  binSnpsSortedvalues=[x[1] for x in binSNPsSorted]\n",
    "  totalNumbers=len(binScores)\n",
    "  for i in range(len(binScoresSorted)):\n",
    "    \n",
    "    threshold=binThresholds[i]\n",
    "    scores=[sum(x) for x in zip(*binScoresSortedvalues[:(i+1)])]\n",
    "    calls=[sum(x) for x in zip(*binCallsSortedvalues[:(i+1)])]\n",
    "    normalizedScores=[score/call for score, call in zip(scores, calls)]\n",
    "    \n",
    "    prsResults[threshold]=[calls,normalizedScores]\n",
    "    combinedSNPs=reduce(lambda x,y: x+y, binSnpsSortedvalues[:(i+1)])\n",
    "    snpNames[threshold]=combinedSNPs\n",
    "    print(\"Processed {} / {} scores\".format(i+1, totalNumbers))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "  return prsResults, snpNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 6, 12]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[2,3,2,4]\n",
    "b=[4,3,2,4]\n",
    "c=[4,3,2,4]\n",
    "reduce(lambda x,y:  map(add, x,y),[a,b,c])\n",
    "#[score/call for score, call in zip(a[1], b[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 11:58:30,963 INFO : Start gathering scores from each bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 / 9 scores\n",
      "Processed 2 / 9 scores\n",
      "Processed 3 / 9 scores\n",
      "Processed 4 / 9 scores\n",
      "Processed 5 / 9 scores\n",
      "Processed 6 / 9 scores\n",
      "Processed 7 / 9 scores\n",
      "Processed 8 / 9 scores\n",
      "Processed 9 / 9 scores\n"
     ]
    }
   ],
   "source": [
    "prsDict, snpids=gatherScores(scoresBin, callsBin, snpBin, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 11:58:53,670 INFO : Successfully output log to ../TestResult.snplog\n"
     ]
    }
   ],
   "source": [
    "# log which SNPs are used in PRS\n",
    "if snp_log:\n",
    "    if flagMap:\n",
    "        logoutput=writeSNPlog(snpids, outputPath, logger, flagMap=flagMap)\n",
    "    else:\n",
    "        logoutput=writeSNPlog(snpids, outputPath, logger)\n",
    "\n",
    "# generate labels for samples\n",
    "#if filetype.lower()==\"vcf\":\n",
    "    #subjNames=genodata.filter(lambda line: \"#CHROM\" in line).map(lambda line: line.split(GENO_delim)[9::]).collect()[0]\n",
    "    #output=writePRS(prsDict,  outputPath, samplenames=subjNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 11:59:41,406 INFO : Collected 260 sample labels\n",
      "2017-03-13 11:59:41,410 INFO : Successfully wrote scores to ../TestResult.score\n"
     ]
    }
   ],
   "source": [
    "if sampleFilePath!=\"NOSAMPLE\":\n",
    "    # get sample name from the provided sample file\n",
    "    subjNames=getSampleNames(sampleFilePath,sampleFileDelim,sampleFileID, skip=sample_skip)\n",
    "\n",
    "    output=writePRS(prsDict,  outputPath, logger, samplenames=subjNames)\n",
    "else:\n",
    "    output=writePRS(prsDict,  outputPath,logger=logger, samplenames=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'totalstart' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-299-c83378d05a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#sc.stop()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtotalstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'totalstart' is not defined"
     ]
    }
   ],
   "source": [
    "if pheno_file is not None:\n",
    "    phenotypes, thresholds, r2All, pAll=regression(prsDict,pheno_file, pheno_delim, pheno_columns, pheno_no_header, covarColumns=covar_columns, outputName=outputPath, logger=logger)\n",
    "\n",
    "    r_square_plots(phenotypes,r2All,pAll, thresholds, outputName=outputPath, width = 3,bar_width = step)\n",
    "\n",
    "#sc.stop()\n",
    "seconds=time.time()-totalstart\n",
    "m, s = divmod(seconds, 60)\n",
    "h, m = divmod(m, 60)\n",
    "logger.info(\"Total Calculation Time : {:d} hrs {:02d} min {:02d} sec\".format(int(h), int(m), int(round(s))))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
